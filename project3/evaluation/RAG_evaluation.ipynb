{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78a84c9-41e4-41cf-b4ce-f5589fe3c227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in /opt/conda/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: bert_score in /opt/conda/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.11/site-packages (1.52.2)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.11/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from rouge-score) (1.24.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from bert_score) (2.5.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from bert_score) (2.1.1)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from bert_score) (4.46.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.11/site-packages (from bert_score) (4.66.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from bert_score) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from bert_score) (23.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from evaluate) (3.0.2)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.9.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.26.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert_score) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->bert_score) (2.0.7)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert_score) (0.20.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert_score) (3.1.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge-score bert_score evaluate accelerate openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d31140-eb1a-4459-b57d-4c8433e8202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union, Any\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bert_score import score as bert_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import evaluate\n",
    "import json\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from elasticsearch import Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986f8df-90ba-4ad3-9242-54677474f29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbaed9f3-8bf6-4104-946d-636d790365fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGDataPreparator:\n",
    "    def __init__(self, data_path: str):\n",
    "        \"\"\"Initialize with path to intents.json\"\"\"\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.raw_data = json.load(f)\n",
    "            \n",
    "    def prepare_evaluation_data(self, num_samples: int = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Prepare data for RAG evaluation\n",
    "        \"\"\"\n",
    "        test_cases = []\n",
    "        \n",
    "        # Get all intents\n",
    "        intents = self.raw_data['intents']\n",
    "        \n",
    "        # Sample intents if num_samples specified\n",
    "        if num_samples and num_samples < len(intents):\n",
    "            selected_intents = random.sample(intents, num_samples)\n",
    "        else:\n",
    "            selected_intents = intents\n",
    "            \n",
    "        for intent in selected_intents:\n",
    "            # For each intent, create test cases from patterns and responses\n",
    "            patterns = intent['patterns']\n",
    "            responses = intent['responses']\n",
    "            \n",
    "            # Create test case from each pattern\n",
    "            for pattern in patterns:\n",
    "                # Randomly select one response as ground truth\n",
    "                ground_truth = random.choice(responses)\n",
    "                \n",
    "                # Create test case\n",
    "                test_case = {\n",
    "                    'query': pattern,\n",
    "                    'ground_truth': ground_truth,\n",
    "                    'tag': intent['tag'],\n",
    "                    # We'll fill these later when we have the RAG system\n",
    "                    'contexts': [],  \n",
    "                    'generated_response': None\n",
    "                }\n",
    "                test_cases.append(test_case)\n",
    "        \n",
    "        return test_cases\n",
    "    \n",
    "    def get_contexts_from_elastic(self, es_client, index_name: str, query: str, k: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get contexts from Elasticsearch for a given query\n",
    "        \"\"\"\n",
    "        search_query = {\n",
    "            \"size\": k,\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": [\"text\", \"original_text\"],\n",
    "                    \"type\": \"best_fields\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = es_client.search(index=index_name, body=search_query)\n",
    "            contexts = [hit['_source'].get('text', '') for hit in response['hits']['hits']]\n",
    "            return contexts\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving contexts: {str(e)}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ec5b95-569c-4a30-a1ce-64036e480fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Test Case:\n",
      "{\n",
      "  \"query\": \"What's the difference between sadness and depression?\",\n",
      "  \"ground_truth\": \"Sadness is a normal reaction to a loss, disappointment, problems, or other difficult situations. Feeling sad from time to time is just another part of being human. In these cases, feelings of sadness go away quickly and you can go about your daily life. Other ways to talk about sadness might be feeling low, feeling down, or feeling blue.A person may say they are feeling depressed, but if it goes away on its own and doesn't impact life in a big way, it probably isn't the illness of depression. Depression is a mental illness that affects your mood, the way you understand yourself, and the way you understand and relate to things around you. It can also go by different names, such as clinical depression, major depressive disorder, or major depression. Depression can come up for no reason, and it lasts for a long time. It's much more than sadness or low mood. People who experience depression may feel worthless or hopeless. They may feel unreasonable guilty. Some people may experience depression as anger or irritability. It may be hard to concentrate or make decisions. Most people lose interest in things that they used to enjoy and may isolate themselves from others. There are also physical signs of depression, such as problems with sleep, appetite and energy and unexplainable aches or pains. Some may experience difficult thoughts about death or ending their life (suicide). Depression lasts longer than two weeks, doesn't usually go away on its own, and impacts your life. It's a real illness, and it is very treatable. It's important to seek help if you're concerned about depression.\",\n",
      "  \"tag\": \"fact-32\",\n",
      "  \"contexts\": [],\n",
      "  \"generated_response\": null\n",
      "}\n",
      "\n",
      "Dataset Statistics:\n",
      "Total test cases: 7\n",
      "Unique intent tags: 3\n"
     ]
    }
   ],
   "source": [
    "preparator = RAGDataPreparator('/home/jovyan/data/intents.json')\n",
    "\n",
    "# Get test cases\n",
    "test_cases = preparator.prepare_evaluation_data(num_samples=3)\n",
    "\n",
    "# Print sample test case\n",
    "print(\"\\nSample Test Case:\")\n",
    "print(json.dumps(test_cases[0], indent=2))\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total test cases: {len(test_cases)}\")\n",
    "unique_tags = len(set(case['tag'] for case in test_cases))\n",
    "print(f\"Unique intent tags: {unique_tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b479a664-2287-429f-8b6e-564ff118dacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282f414-e6ab-4985-ac5b-6698456b24da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce694d24-abbf-4383-8e45-503122a0f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveRAGEvaluator:\n",
    "    def __init__(self, llm_client, rag_system):\n",
    "        \"\"\"Initialize with better error handling\"\"\"\n",
    "        self.llm = llm_client\n",
    "        self.rag_system = rag_system\n",
    "        self.model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "        # Initialize NLTK's BLEU with smoothing\n",
    "        from nltk.translate.bleu_score import SmoothingFunction\n",
    "        self.smoothing = SmoothingFunction().method1\n",
    "        \n",
    "        # Initialize logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def evaluate_traditional_metrics(self, \n",
    "                                  query: str,\n",
    "                                  generated_response: str,\n",
    "                                  ground_truth: str,\n",
    "                                  retrieved_contexts: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Traditional metrics with improved BLEU calculation\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        try:\n",
    "            # ROUGE Scores\n",
    "            rouge_scores = self.rouge_scorer.score(ground_truth, generated_response)\n",
    "            metrics['rouge1_f1'] = rouge_scores['rouge1'].fmeasure\n",
    "            metrics['rouge2_f1'] = rouge_scores['rouge2'].fmeasure\n",
    "            metrics['rougeL_f1'] = rouge_scores['rougeL'].fmeasure\n",
    "            \n",
    "            # BLEU Score with smoothing\n",
    "            metrics['bleu'] = sentence_bleu(\n",
    "                [ground_truth.split()],\n",
    "                generated_response.split(),\n",
    "                smoothing_function=self.smoothing\n",
    "            )\n",
    "            \n",
    "            # Semantic Similarity using SentenceTransformer\n",
    "            ground_truth_embedding = self.model.encode(ground_truth)\n",
    "            response_embedding = self.model.encode(generated_response)\n",
    "            \n",
    "            similarity = cosine_similarity(\n",
    "                [ground_truth_embedding],\n",
    "                [response_embedding]\n",
    "            )[0][0]\n",
    "            metrics['semantic_similarity'] = float(similarity)\n",
    "            \n",
    "            # Context Relevance (if contexts available)\n",
    "            if retrieved_contexts:\n",
    "                query_embedding = self.model.encode(query)\n",
    "                context_embeddings = self.model.encode(retrieved_contexts)\n",
    "                context_similarities = cosine_similarity(\n",
    "                    [query_embedding],\n",
    "                    context_embeddings\n",
    "                )[0]\n",
    "                metrics['context_relevance'] = float(np.mean(context_similarities))\n",
    "            else:\n",
    "                metrics['context_relevance'] = 0.0\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in metric calculation: {str(e)}\")\n",
    "            metrics = {\n",
    "                'rouge1_f1': 0.0,\n",
    "                'rouge2_f1': 0.0,\n",
    "                'rougeL_f1': 0.0,\n",
    "                'bleu': 0.0,\n",
    "                'semantic_similarity': 0.0,\n",
    "                'context_relevance': 0.0\n",
    "            }\n",
    "            \n",
    "        return metrics\n",
    "\n",
    "    def _extract_json_from_text(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Enhanced JSON extraction and cleaning\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # If text is already a dict, return it\n",
    "            if isinstance(text, dict):\n",
    "                return text\n",
    "                \n",
    "            # Find JSON pattern\n",
    "            import re\n",
    "            json_pattern = r'\\{[\\s\\S]*\\}'\n",
    "            match = re.search(json_pattern, text)\n",
    "            \n",
    "            if match:\n",
    "                json_str = match.group()\n",
    "                \n",
    "                # Clean up JSON string\n",
    "                json_str = (\n",
    "                    json_str\n",
    "                    .replace(\"'\", '\"')  # Replace single quotes\n",
    "                    .replace('\\n', ' ')  # Remove newlines\n",
    "                    .replace('None', 'null')  # Replace Python None\n",
    "                )\n",
    "                \n",
    "                # Remove trailing commas before closing braces/brackets\n",
    "                json_str = re.sub(r',\\s*([\\]}])', r'\\1', json_str)\n",
    "                \n",
    "                # Clean up any double commas\n",
    "                json_str = re.sub(r',\\s*,', ',', json_str)\n",
    "                \n",
    "                # Remove comments if any\n",
    "                json_str = re.sub(r'//.*?[\\n\\r]', '', json_str)\n",
    "                json_str = re.sub(r'/\\*.*?\\*/', '', json_str, flags=re.DOTALL)\n",
    "                \n",
    "                try:\n",
    "                    # Try parsing the cleaned JSON\n",
    "                    return json.loads(json_str)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    # If still failing, try more aggressive cleaning\n",
    "                    # Remove all whitespace between brackets\n",
    "                    json_str = re.sub(r'\\s+(?=[^\"]*(?:\"[^\"]*\"[^\"]*)*$)', '', json_str)\n",
    "                    # Ensure property names are quoted\n",
    "                    json_str = re.sub(r'([{,])\\s*([a-zA-Z0-9_]+):', r'\\1\"\\2\":', json_str)\n",
    "                    return json.loads(json_str)\n",
    "            else:\n",
    "                raise ValueError(\"No JSON object found in response\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"JSON extraction failed: {str(e)}\\nOriginal text: {text}\")\n",
    "            # Return a valid default structure\n",
    "            return self._get_default_evaluation()\n",
    "    \n",
    "    def llm_judge_evaluation(self,\n",
    "                           query: str,\n",
    "                           generated_response: str,\n",
    "                           ground_truth: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Improved LLM evaluation with stricter JSON formatting\n",
    "        \"\"\"\n",
    "        try:\n",
    "            evaluation_prompt = f\"\"\"Evaluate this Q&A interaction. Respond with a JSON object exactly matching this structure:\n",
    "    \n",
    "    Question: {query}\n",
    "    Generated Response: {generated_response}\n",
    "    {\"Reference Answer: \" + ground_truth if ground_truth else \"\"}\n",
    "    \n",
    "    Required JSON structure:\n",
    "    {{\n",
    "        \"scores\": {{\n",
    "            \"relevance\": {{\"score\": 0, \"explanation\": \"text\"}},\n",
    "            \"accuracy\": {{\"score\": 0, \"explanation\": \"text\"}},\n",
    "            \"completeness\": {{\"score\": 0, \"explanation\": \"text\"}},\n",
    "            \"clarity\": {{\"score\": 0, \"explanation\": \"text\"}}\n",
    "        }},\n",
    "        \"overall_score\": 0,\n",
    "        \"feedback\": \"text\"\n",
    "    }}\n",
    "    \n",
    "    Ensure:\n",
    "    - No trailing commas\n",
    "    - All property names in double quotes\n",
    "    - No comments\n",
    "    - Valid JSON only\n",
    "    \"\"\"\n",
    "    \n",
    "            response = self.llm.generate(evaluation_prompt)\n",
    "            evaluation = self._extract_json_from_text(response)\n",
    "            \n",
    "            # Validate structure\n",
    "            if not self._validate_evaluation_structure(evaluation):\n",
    "                self.logger.warning(\"Invalid evaluation structure, using default\")\n",
    "                return self._get_default_evaluation()\n",
    "                \n",
    "            return evaluation\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"LLM evaluation failed: {str(e)}\")\n",
    "            return self._get_default_evaluation()\n",
    "\n",
    "    def _get_default_evaluation(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return default evaluation when LLM evaluation fails\"\"\"\n",
    "        return {\n",
    "            \"scores\": {\n",
    "                \"relevance\": {\"score\": 0, \"explanation\": \"Evaluation failed\"},\n",
    "                \"accuracy\": {\"score\": 0, \"explanation\": \"Evaluation failed\"},\n",
    "                \"completeness\": {\"score\": 0, \"explanation\": \"Evaluation failed\"},\n",
    "                \"clarity\": {\"score\": 0, \"explanation\": \"Evaluation failed\"}\n",
    "            },\n",
    "            \"overall_score\": 0,\n",
    "            \"feedback\": \"Evaluation failed\"\n",
    "        }\n",
    "\n",
    "    def _validate_evaluation_structure(self, evaluation: Dict) -> bool:\n",
    "        \"\"\"\n",
    "        Validate the evaluation structure\n",
    "        \"\"\"\n",
    "        try:\n",
    "            required_fields = {\n",
    "                'scores': {\n",
    "                    'relevance': ['score', 'explanation'],\n",
    "                    'accuracy': ['score', 'explanation'],\n",
    "                    'completeness': ['score', 'explanation'],\n",
    "                    'clarity': ['score', 'explanation']\n",
    "                },\n",
    "                'overall_score': None,\n",
    "                'feedback': None\n",
    "            }\n",
    "            \n",
    "            # Check main fields\n",
    "            if not all(field in evaluation for field in required_fields):\n",
    "                return False\n",
    "                \n",
    "            # Check scores structure\n",
    "            scores = evaluation.get('scores', {})\n",
    "            for category, fields in required_fields['scores'].items():\n",
    "                if category not in scores:\n",
    "                    return False\n",
    "                if fields:\n",
    "                    if not all(field in scores[category] for field in fields):\n",
    "                        return False\n",
    "                        \n",
    "            return True\n",
    "            \n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def aqa_evaluation(self, original_answer: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Improved AQA evaluation with stricter JSON handling\n",
    "        \"\"\"\n",
    "        try:\n",
    "            comparison_prompt = f\"\"\"Compare these answers and provide scores as a JSON object:\n",
    "    \n",
    "    Original Answer: {original_answer}\n",
    "    Generated Answer: {self.rag_system.generate_response(original_answer)}\n",
    "    \n",
    "    Required JSON structure:\n",
    "    {{\n",
    "        \"semantic_similarity\": 0.0,\n",
    "        \"factual_consistency\": 0.0,\n",
    "        \"information_coverage\": 0.0\n",
    "    }}\n",
    "    \n",
    "    Ensure:\n",
    "    - Scores between 0.0 and 1.0\n",
    "    - No trailing commas\n",
    "    - All property names in double quotes\n",
    "    - Valid JSON only\"\"\"\n",
    "    \n",
    "            response = self.llm.generate(comparison_prompt)\n",
    "            scores = self._extract_json_from_text(response)\n",
    "            \n",
    "            # Validate and normalize scores\n",
    "            return self._normalize_scores(scores)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"AQA evaluation failed: {str(e)}\")\n",
    "            return {\n",
    "                \"semantic_similarity\": 0.0,\n",
    "                \"factual_consistency\": 0.0,\n",
    "                \"information_coverage\": 0.0\n",
    "            }\n",
    "\n",
    "\n",
    "    def _normalize_scores(self, scores: Dict) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Normalize scores to ensure they're valid floats between 0 and 1\n",
    "        \"\"\"\n",
    "        required_fields = [\"semantic_similarity\", \"factual_consistency\", \"information_coverage\"]\n",
    "        normalized = {}\n",
    "        \n",
    "        for field in required_fields:\n",
    "            try:\n",
    "                value = float(scores.get(field, 0))\n",
    "                normalized[field] = max(0.0, min(1.0, value))\n",
    "            except (TypeError, ValueError):\n",
    "                normalized[field] = 0.0\n",
    "                \n",
    "        return normalized\n",
    "        \n",
    "    def _get_default_evaluation(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Return default evaluation with proper score formatting\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"scores\": {\n",
    "                \"relevance\": {\"score\": 0, \"explanation\": \"Evaluation failed\"},\n",
    "                \"accuracy\": {\"score\": 0, \"explanation\": \"Evaluation failed\"},\n",
    "                \"completeness\": {\"score\": 0, \"explanation\": \"Evaluation failed\"},\n",
    "                \"clarity\": {\"score\": 0, \"explanation\": \"Evaluation failed\"}\n",
    "            },\n",
    "            \"overall_score\": 0,\n",
    "            \"feedback\": \"Evaluation failed\"\n",
    "        }\n",
    "    \n",
    "    def comprehensive_evaluation(self, \n",
    "                               test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run comprehensive evaluation using all three methods\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'traditional_metrics': [],\n",
    "            'llm_judge_results': [],\n",
    "            'aqa_results': [],\n",
    "            'per_query_results': []\n",
    "        }\n",
    "        \n",
    "        for test_case in tqdm(test_cases, desc=\"Evaluating test cases\"):\n",
    "            query = test_case['query']\n",
    "            ground_truth = test_case.get('ground_truth')\n",
    "            contexts = test_case.get('contexts', [])\n",
    "            \n",
    "            # Get RAG response if not provided\n",
    "            generated_response = test_case.get('generated_response') or \\\n",
    "                               self.rag_system.generate_response(query)\n",
    "            \n",
    "            # 1. Traditional Metrics\n",
    "            if ground_truth:\n",
    "                trad_metrics = self.evaluate_traditional_metrics(\n",
    "                    query, generated_response, ground_truth, contexts\n",
    "                )\n",
    "            else:\n",
    "                trad_metrics = None\n",
    "            \n",
    "            # 2. LLM Judge\n",
    "            llm_evaluation = self.llm_judge_evaluation(\n",
    "                query, generated_response, ground_truth\n",
    "            )\n",
    "            \n",
    "            # 3. AQA Evaluation\n",
    "            if ground_truth:\n",
    "                aqa_results = self.aqa_evaluation(ground_truth)\n",
    "            else:\n",
    "                aqa_results = None\n",
    "            \n",
    "            # Store all results\n",
    "            results['traditional_metrics'].append(trad_metrics)\n",
    "            results['llm_judge_results'].append(llm_evaluation)\n",
    "            results['aqa_results'].append(aqa_results)\n",
    "            \n",
    "            # Store per-query results\n",
    "            results['per_query_results'].append({\n",
    "                'query': query,\n",
    "                'generated_response': generated_response,\n",
    "                'ground_truth': ground_truth,\n",
    "                'traditional_metrics': trad_metrics,\n",
    "                'llm_evaluation': llm_evaluation,\n",
    "                'aqa_results': aqa_results\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def generate_report(self, results: Dict[str, Any], output_file: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate evaluation report with better error handling and type conversion\n",
    "        \"\"\"\n",
    "        if not results:\n",
    "            logger.warning(\"No evaluation results available\")\n",
    "            return \"No results to report\"\n",
    "            \n",
    "        report = [\"# RAG System Comprehensive Evaluation Report\"]\n",
    "        report.append(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "        \n",
    "        # 1. Traditional Metrics Summary\n",
    "        report.append(\"\\n## Traditional Metrics Summary\")\n",
    "        if results.get('traditional_metrics'):\n",
    "            trad_metrics_df = pd.DataFrame([r for r in results['traditional_metrics'] if r is not None])\n",
    "            if not trad_metrics_df.empty:\n",
    "                report.append(\"\\nAverage Scores:\")\n",
    "                # Convert to numeric, replacing non-numeric values with 0\n",
    "                numeric_means = trad_metrics_df.apply(pd.to_numeric, errors='coerce').mean()\n",
    "                report.append(numeric_means.to_string())\n",
    "        \n",
    "        # 2. LLM Judge Summary\n",
    "        report.append(\"\\n## LLM Judge Evaluation Summary\")\n",
    "        llm_scores = []\n",
    "        critiques = []\n",
    "        for r in results.get('llm_judge_results', []):\n",
    "            if r and isinstance(r, dict):\n",
    "                if 'overall_score' in r:\n",
    "                    try:\n",
    "                        score = float(r['overall_score'])\n",
    "                        llm_scores.append(score)\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "                if 'feedback' in r:\n",
    "                    critiques.append(str(r['feedback']))\n",
    "        \n",
    "        if llm_scores:\n",
    "            report.append(f\"\\nAverage Overall Score: {np.mean(llm_scores):.2f}\")\n",
    "        if critiques:\n",
    "            report.append(\"\\nSample Feedback:\")\n",
    "            report.extend([f\"- {c}\" for c in critiques[:3]])  # Show only first 3 critiques\n",
    "        \n",
    "        # 3. AQA Results Summary\n",
    "        report.append(\"\\n## A→Q→A' Evaluation Summary\")\n",
    "        if results.get('aqa_results'):\n",
    "            aqa_metrics = []\n",
    "            for r in results['aqa_results']:\n",
    "                if isinstance(r, dict):\n",
    "                    # Convert all values to float or use 0.0\n",
    "                    metrics = {}\n",
    "                    for k, v in r.items():\n",
    "                        try:\n",
    "                            metrics[k] = float(v)\n",
    "                        except (ValueError, TypeError):\n",
    "                            metrics[k] = 0.0\n",
    "                    aqa_metrics.append(metrics)\n",
    "            \n",
    "            if aqa_metrics:\n",
    "                aqa_df = pd.DataFrame(aqa_metrics)\n",
    "                report.append(\"\\nAverage Scores:\")\n",
    "                report.append(aqa_df.mean().to_string())\n",
    "        \n",
    "        # 4. Detailed Analysis\n",
    "        report.append(\"\\n## Detailed Query Analysis\")\n",
    "        if results.get('per_query_results'):\n",
    "            for idx, query_result in enumerate(results['per_query_results'], 1):\n",
    "                if isinstance(query_result, dict):\n",
    "                    report.append(f\"\\n### Query {idx}\")\n",
    "                    report.append(f\"Query: {query_result.get('query', 'N/A')}\")\n",
    "                    if 'generated_response' in query_result:\n",
    "                        report.append(f\"Generated Response: {query_result['generated_response']}\")\n",
    "                    if 'ground_truth' in query_result:\n",
    "                        report.append(f\"Ground Truth: {query_result['ground_truth']}\")\n",
    "        \n",
    "        # Save report\n",
    "        report_text = \"\\n\".join(report)\n",
    "        if output_file:\n",
    "            try:\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(report_text)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error saving report: {str(e)}\")\n",
    "        \n",
    "        return report_text\n",
    "\n",
    "    def _create_visualizations(self, results: Dict[str, Any], output_prefix: str = None):\n",
    "        \"\"\"\n",
    "        Create visualization plots for the results\n",
    "        \"\"\"\n",
    "        # 1. Traditional Metrics Distribution\n",
    "        trad_metrics_df = pd.DataFrame([r for r in results['traditional_metrics'] if r is not None])\n",
    "        if not trad_metrics_df.empty:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.boxplot(data=trad_metrics_df)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.title(\"Distribution of Traditional Metrics\")\n",
    "            if output_prefix:\n",
    "                plt.savefig(f\"{output_prefix}_traditional_metrics.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        # 2. LLM Judge Scores\n",
    "        llm_scores = [r['overall_score'] for r in results['llm_judge_results'] if r and 'overall_score' in r]\n",
    "        if llm_scores:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.histplot(llm_scores, bins=10)\n",
    "            plt.title(\"Distribution of LLM Judge Scores\")\n",
    "            if output_prefix:\n",
    "                plt.savefig(f\"{output_prefix}_llm_scores.png\")\n",
    "            plt.close()\n",
    "        \n",
    "        # 3. AQA Metrics\n",
    "        aqa_metrics_df = pd.DataFrame([r for r in results['aqa_results'] if r is not None])\n",
    "        if not aqa_metrics_df.empty:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.boxplot(data=aqa_metrics_df[['semantic_similarity', 'factual_consistency', 'information_coverage']])\n",
    "            plt.title(\"A→Q→A' Evaluation Metrics\")\n",
    "            if output_prefix:\n",
    "                plt.savefig(f\"{output_prefix}_aqa_metrics.png\")\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa528d39-3dea-4cd6-85d4-77389f807490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalLLMClient:\n",
    "    def __init__(self, model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
    "        \"\"\"Initialize local LLM pipeline\"\"\"\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate response using local LLM\"\"\"\n",
    "        try:\n",
    "            response = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "            return response[0]['generated_text']\n",
    "        except Exception as e:\n",
    "            print(f\"Error in generation: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, es_client, llm_client):\n",
    "        \"\"\"Initialize RAG system\"\"\"\n",
    "        self.es = es_client\n",
    "        self.llm = llm_client\n",
    "    \n",
    "    def generate_response(self, query: str) -> str:\n",
    "        \"\"\"Generate RAG response\"\"\"\n",
    "        try:\n",
    "            # 1. Retrieve contexts\n",
    "            contexts = self._retrieve_contexts(query)\n",
    "            \n",
    "            # 2. Create prompt\n",
    "            prompt = self._create_prompt(query, contexts)\n",
    "            \n",
    "            # 3. Generate response\n",
    "            response = self.llm.generate(prompt)\n",
    "            \n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error in RAG response: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _retrieve_contexts(self, query: str, k: int = 3) -> list:\n",
    "        \"\"\"Retrieve relevant contexts from Elasticsearch\"\"\"\n",
    "        try:\n",
    "            search_query = {\n",
    "                \"size\": k,\n",
    "                \"query\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"text\", \"original_text\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = self.es.search(index=\"qa_index_2\", body=search_query)\n",
    "            return [hit['_source'].get('text', '') for hit in response['hits']['hits']]\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving contexts: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def _create_prompt(self, query: str, contexts: list) -> str:\n",
    "        \"\"\"Create prompt with contexts\"\"\"\n",
    "        context_text = \" \".join(contexts)\n",
    "        return f\"\"\"<|system|>\n",
    "Use this context to answer: {context_text}\n",
    "\n",
    "<|user|>\n",
    "{query}\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba0c5c4-b356-4056-9903-2abdf588e67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd0f48-4f04-4199-a0e5-11ab2af530a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab5564a8-8d5d-4337-af45-fcf51fa89494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch successfully!\n",
      "{'qa_index': {'mappings': {'properties': {'question_text_vector_knn': {'type': 'dense_vector', 'dims': 768, 'index': True, 'similarity': 'cosine', 'index_options': {'type': 'int8_hnsw', 'm': 16, 'ef_construction': 100}}, 'question_vector_knn': {'type': 'dense_vector', 'dims': 768, 'index': True, 'similarity': 'cosine', 'index_options': {'type': 'int8_hnsw', 'm': 16, 'ef_construction': 100}}, 'response': {'type': 'text'}, 'text': {'type': 'text', 'analyzer': 'rag_analyzer'}, 'text_vector_knn': {'type': 'dense_vector', 'dims': 768, 'index': True, 'similarity': 'cosine', 'index_options': {'type': 'int8_hnsw', 'm': 16, 'ef_construction': 100}}, 'vector_combined_knn': {'type': 'dense_vector', 'dims': 768, 'index': True, 'similarity': 'cosine', 'index_options': {'type': 'int8_hnsw', 'm': 16, 'ef_construction': 100}}}}}}\n",
      "{'question_text_vector_knn': {'type': 'dense_vector', 'dims': 768, 'index': True, 'similarity': 'cosine', 'index_options': {'type': 'int8_hnsw', 'm': 16, 'ef_construction': 100}}, 'question_vector_knn': {'type': 'dense_vector', 'dims': 768, 'index': True, 'similarity': 'cosine', 'index_options': {'type': 'int8_hnsw', 'm': 16, 'ef_construction': 100}}, 'response': {'type': 'text'}, 'text': {'type': 'text', 'analyzer': 'rag_analyzer'}, 'text_vector_knn': {'type': 'dense_vector', 'dims': 768, 'index': True, 'similarity': 'cosine', 'index_options': {'type': 'int8_hnsw', 'm': 16, 'ef_construction': 100}}, 'vector_combined_knn': {'type': 'dense_vector', 'dims': 768, 'index': True, 'similarity': 'cosine', 'index_options': {'type': 'int8_hnsw', 'm': 16, 'ef_construction': 100}}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize Elasticsearch client\n",
    "es = Elasticsearch(\"http://elasticsearch:9200\")\n",
    "\n",
    "# Quick connection test\n",
    "if es.ping():\n",
    "    print(\"Connected to Elasticsearch successfully!\")\n",
    "else:\n",
    "    print(\"Failed to connect to Elasticsearch.\")\n",
    "\n",
    "# Retrieve mappings\n",
    "index_mappings = es.indices.get_mapping(index=\"qa_index\")\n",
    "print(index_mappings)\n",
    "\n",
    "print(index_mappings['qa_index']['mappings']['properties'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a245ea-bf10-41d8-a68b-f5c4004a4922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f4c600b-22e7-4a2e-a8ba-383c364127f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class OpenAIClient:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # or \"gpt-4\" for better evaluation\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator of question-answering systems.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "274bb99c-05a8-4bbd-845e-a82d573d4568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test cases:  57%|█████▋    | 4/7 [00:27<00:20,  6.91s/it]ERROR:__main__:JSON extraction failed: Expecting ',' delimiter: line 1 column 119 (char 118)\n",
      "Original text: {\n",
      "    \"scores\": {\n",
      "        \"relevance\": {\"score\": 0, \"explanation\": \"The generated response does not directly address the user's request for advice.\"},\n",
      "        \"accuracy\": {\"score\": 1, \"explanation\": \"The information provided is accurate and relevant to mental health concerns.\"},\n",
      "        \"completeness\": {\"score\": 1, \"explanation\": \"The response covers warning signs of mental illness and the importance of seeking professional help.\"},\n",
      "        \"clarity\": {\"score\": 1, \"explanation\": \"The response is clear and well-structured, providing information in a supportive manner.\"}\n",
      "    },\n",
      "    \"overall_score\": 0.75,\n",
      "    \"feedback\": \"While the generated response provides valuable information on mental health and the importance of seeking help, it does not directly address the user's request for advice. Consider incorporating specific advice or suggestions in the response to better align with the user's initial query.\"\n",
      "}\n",
      "Evaluating test cases: 100%|██████████| 7/7 [00:46<00:00,  6.69s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# Initialize all components\n",
    "llm_client = OpenAIClient(api_key=\"YOUR-OPENAI-KEY\") # Using TinyLlama\n",
    "rag_system = RAGSystem(es, llm_client)\n",
    "evaluator = ComprehensiveRAGEvaluator(llm_client, rag_system)\n",
    "\n",
    "# Get contexts\n",
    "for test_case in test_cases:\n",
    "    contexts = preparator.get_contexts_from_elastic(\n",
    "        es,\n",
    "        \"qa_index_2\",\n",
    "        test_case['query']\n",
    "    )\n",
    "    test_case['contexts'] = contexts\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.comprehensive_evaluation(test_cases)\n",
    "\n",
    "\n",
    "# Generate report\n",
    "report = evaluator.generate_report(results, \"rag_evaluation_report_gpt35_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab28045-a07f-4514-b61a-a71571c94424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623c225-b459-4d04-8ab3-37cac1bc595a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc41be2-7144-44c5-a9a6-b02aaa325ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class OpenAIClient:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",  # or \"gpt-4\" for better evaluation\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator of question-answering systems.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "364079fc-33cc-4969-9487-664737784389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test cases:  57%|█████▋    | 4/7 [00:46<00:32, 10.96s/it]ERROR:__main__:JSON extraction failed: Expecting ',' delimiter: line 1 column 923 (char 922)\n",
      "Original text: ```json\n",
      "{\n",
      "    \"scores\": {\n",
      "        \"relevance\": {\n",
      "            \"score\": 1,\n",
      "            \"explanation\": \"The response focuses on providing advice related to mental health, which is a specific interpretation of the general request for advice, indicating a narrow but relevant response.\"\n",
      "        },\n",
      "        \"accuracy\": {\n",
      "            \"score\": 2,\n",
      "            \"explanation\": \"The given information accurately lists common warning signs of mental illness and advises seeking professional help, which is generally considered correct advice.\"\n",
      "        },\n",
      "        \"completeness\": {\n",
      "            \"score\": 1,\n",
      "            \"explanation\": \"The response provides a detailed list of signs of mental illness but does not address other types of advice that the user might be seeking.\"\n",
      "        },\n",
      "        \"clarity\": {\n",
      "            \"score\": 2,\n",
      "            \"explanation\": \"The response clearly lists the signs and symptoms, making it easy for the user to understand the information provided.\"\n",
      "        }\n",
      "    },\n",
      "    \"overall_score\": 1,\n",
      "    \"feedback\": \"The response provides accurate and clear advice related to mental health but may not be directly relevant to the user's unspecified request for general advice. It would be beneficial to ask clarifying questions to better cater to the user's needs.\"\n",
      "}\n",
      "```\n",
      "Evaluating test cases:  71%|███████▏  | 5/7 [00:59<00:23, 11.70s/it]ERROR:__main__:JSON extraction failed: Expecting ',' delimiter: line 1 column 165 (char 164)\n",
      "Original text: ```json\n",
      "{\n",
      "    \"scores\": {\n",
      "        \"relevance\": {\n",
      "            \"score\": 3,\n",
      "            \"explanation\": \"The response is related to mental health advice, but the question was vague and didn't specify mental health specifically.\"\n",
      "        },\n",
      "        \"accuracy\": {\n",
      "            \"score\": 4,\n",
      "            \"explanation\": \"The response provides accurate information regarding mental health treatment and advice.\"\n",
      "        },\n",
      "        \"completeness\": {\n",
      "            \"score\": 4,\n",
      "            \"explanation\": \"The response thoroughly covers aspects of mental health treatment, including diagnosis, support systems, and emergency plans.\"\n",
      "        },\n",
      "        \"clarity\": {\n",
      "            \"score\": 4,\n",
      "            \"explanation\": \"The response is clear and well-structured, offering advice in a logical and concise manner.\"\n",
      "        }\n",
      "    },\n",
      "    \"overall_score\": 4,\n",
      "    \"feedback\": \"The response is well-informed and provides comprehensive advice on mental health, although it would have benefited from more context or specificity regarding the nature of the advice needed.\"\n",
      "}\n",
      "```\n",
      "Evaluating test cases: 100%|██████████| 7/7 [01:24<00:00, 12.06s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# Initialize all components\n",
    "llm_client = OpenAIClient(api_key=\"YOUR-OPENAI-KEY\") # Using TinyLlama\n",
    "rag_system = RAGSystem(es, llm_client)\n",
    "evaluator = ComprehensiveRAGEvaluator(llm_client, rag_system)\n",
    "\n",
    "# Get contexts\n",
    "for test_case in test_cases:\n",
    "    contexts = preparator.get_contexts_from_elastic(\n",
    "        es,\n",
    "        \"qa_index_2\",\n",
    "        test_case['query']\n",
    "    )\n",
    "    test_case['contexts'] = contexts\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.comprehensive_evaluation(test_cases)\n",
    "\n",
    "\n",
    "# Generate report\n",
    "report = evaluator.generate_report(results, \"rag_evaluation_report_gpt4o_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a9db2-e133-41d5-b539-671a93f044fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d9f5d-206c-43b8-9486-00dfb2cce981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b9746-5c37-44fd-83c3-4cdf4d6597c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3da34-dad8-40ba-be95-69f5ad9ac62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b04968c-ed7a-49ec-ae07-851d06c4cc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34efacac-7fc1-49e1-a8e7-4746371fedbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e9a60-cc6d-46f2-8c6a-cc12ed11208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# Initialize all components\n",
    "llm_client = LocalLLMClient()  # Using TinyLlama\n",
    "rag_system = RAGSystem(es, llm_client)\n",
    "evaluator = ComprehensiveRAGEvaluator(llm_client, rag_system)\n",
    "\n",
    "# Get contexts\n",
    "for test_case in test_cases:\n",
    "    contexts = preparator.get_contexts_from_elastic(\n",
    "        es,\n",
    "        \"qa_index_2\",\n",
    "        test_case['query']\n",
    "    )\n",
    "    test_case['contexts'] = contexts\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.comprehensive_evaluation(test_cases)\n",
    "\n",
    "# Generate report\n",
    "report = evaluator.generate_report(results, \"rag_evaluation_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4130607-35a5-4b6e-8643-4feb4f862687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee21a36-0a66-48b0-8de1-8a6c54e33e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
